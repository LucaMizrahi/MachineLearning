{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos nas aulas anteriores modelos lineares de regressão, onde o valor predito do *target* era modelado como uma combinação linear de atributos (incluindo o termo constante):\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n = \n",
    "\\begin{bmatrix}\n",
    "1 & x_0 & x_1 & x_2 & \\cdots & x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix}\n",
    "= \\utilde{\\mathbf{x}}^T \\mathbf{\\theta}\n",
    "$$\n",
    "\n",
    "Os parâmetros ótimos do nosso modelo são aqueles que minimizam o erro quadrático médio (em inglês: *mean squared error* - MSE):\n",
    "\n",
    "$$\n",
    "\\text{MSE}(\\theta) = \n",
    "\\frac{1}{m} \\sum_{i = 1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)}\\right)^2 = \n",
    "\\frac{1}{m} \\sum_{i = 1}^{m} \\left( (\\utilde{\\mathbf{x}}^{(i)})^T \\theta - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "Em notação matricial:\n",
    "\n",
    "- Conjunto de exemplos: matriz de *features* $\\mathbf{X}$ e matriz-coluna de valores *target* $\\mathbf{y}$\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\left[\n",
    "\\begin{matrix}\n",
    "x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\\, , \\quad\n",
    "\\mathbf{y} = \\left[\n",
    "\\begin{matrix}\n",
    "y^{(1)} \\\\\n",
    "y^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "y^{(m)} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "- Modelo: matriz de *features* aumentada $\\utilde{\\mathbf{X}}$ (a matriz $\\mathbf{X}$ com uma coluna de valores $1$ a mais), matriz-coluna de parâmetros $\\mathbf{\\theta}$, matriz-coluna de valores preditos $\\hat{\\mathbf{y}}$\n",
    "\n",
    "$$\n",
    "\\utilde{\\mathbf{X}} = \\left[\n",
    "\\begin{matrix}\n",
    "1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\\, , \\quad\n",
    "\\mathbf{\\theta} = \\left[\n",
    "\\begin{matrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n \\\\\n",
    "\\end{matrix}\n",
    "\\right]\\, , \\quad\n",
    "\\hat{\\mathbf{y}} = \\left[\n",
    "\\begin{matrix}\n",
    "\\hat{y}^{(1)} \\\\\n",
    "\\hat{y}^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}^{(m)} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \\boxed{\\hat{\\mathbf{y}} = \\utilde{\\mathbf{X}} \\mathbf{\\theta}}\n",
    "$$\n",
    "\n",
    "\n",
    "- Função de perda de regressão: comumente usamos o MSE\n",
    "\n",
    "$$\n",
    "\\text{MSE}(\\mathbf{\\theta}) = \n",
    "\\frac{1}{m}\n",
    "\\left(\\hat{\\mathbf{y}} - \\mathbf{y}\\right)^T\n",
    "\\left(\\hat{\\mathbf{y}} - \\mathbf{y}\\right)\n",
    "$$\n",
    "\n",
    "As vezes, um modelo de regressão pode ser adaptado para um modelo de classificação e vice-versa. Este é o caso da ***regressão logística***, que apesar do nome *não é um método de regressão*, mas sim um método de classificação!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O problema: classificação binária\n",
    "\n",
    "Suponha que tenhamos os seguintes dados que relacionam o número de horas de estudo de um aluno com o resultado de uma prova (0: não passou; 1: passou):\n",
    "\n",
    "| horas| passou? |\n",
    "|---|---|\n",
    "| 3.75 | 0.0 |\n",
    "| 9.51 | 1.0 |\n",
    "| 7.32 | 0.0 |\n",
    "| 5.99 | 1.0 |\n",
    "| 1.56 | 0.0 |\n",
    "| 1.56 | 0.0 |\n",
    "| 0.58 | 0.0 |\n",
    "| 8.66 | 1.0 |\n",
    "| 6.01 | 1.0 |\n",
    "| 7.08 | 1.0 |\n",
    "| 0.21 | 0.0 |\n",
    "| 9.70 | 1.0 |\n",
    "| 8.32 | 0.0 |\n",
    "| 2.12 | 0.0 |\n",
    "| 1.82 | 0.0 |\n",
    "| 1.83 | 0.0 |\n",
    "| 3.04 | 0.0 |\n",
    "| 5.25 | 1.0 |\n",
    "| 4.32 | 0.0 |\n",
    "| 2.91 | 0.0 |\n",
    "| 6.12 | 1.0 |\n",
    "| 1.39 | 0.0 |\n",
    "| 2.92 | 1.0 |\n",
    "| 3.66 | 1.0 |\n",
    "| 4.56 | 0.0 |\n",
    "| 7.85 | 1.0 |\n",
    "| 2.00 | 0.0 |\n",
    "| 5.14 | 0.0 |\n",
    "| 5.92 | 0.0 |\n",
    "| 0.46 | 0.0 |\n",
    "\n",
    "Eis um gráfico para ajudar a visualizar esses dados:\n",
    "\n",
    "![passou ou não](alunos.png \"Resultado do teste versus número de horas de estudo\")\n",
    "\n",
    "Parece que se um aluno não estuda não passa, e se estuda bastante passa. E no meio do caminho? Como estimar a chance de que o aluno passe se estudar $7$ horas, por exemplo? Parece que precisamos de uma função interpoladora aqui! Existem várias opções de função interpoladora, vamos estudar uma delas: a **função logística**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função logística\n",
    "\n",
    "Para adaptar a regressão linear para a regressão logística (que não é regressão, mas sim um método de classificação), precisamos da *função logística*:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Esta função se comporta da seguinte maneira:\n",
    "\n",
    "- Para valores muito negativos de $x$ temos $\\lim_{x \\rightarrow -\\infty} \\sigma(x) = 0$ pois o denominador da fração vai para infinito.\n",
    "\n",
    "- Para valores muito positivos de $x$ temos $\\lim_{x \\rightarrow \\infty} \\sigma(x) = 1$ pois $e^{-x}$ vai para zero.\n",
    "\n",
    "- Para $x = 0$ temos $\\sigma(0) = 0.5$\n",
    "\n",
    "Eis a cara dessa função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera valores da função logística entre -10 e 10.\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plota a função logística.\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Plota linhas auxiliares só para visualizar melhor.\n",
    "plt.plot([0, 0], [0, 1], 'k--', alpha=0.2)\n",
    "plt.plot([-10, 10], [0.5, 0.5], 'k--', alpha=0.2)\n",
    "\n",
    "# Resto do gráfico: titulo, labels, etc.\n",
    "plt.title('Função logística')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$y = \\sigma(x)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função logística tem um formato *sigmoide* (ou seja, em forma de \"s\").\n",
    "\n",
    "---\n",
    "\n",
    "**Atividade:** Como você faria (matematicamente) para:\n",
    "\n",
    "- Deslocar a função logística para a direita?\n",
    "\n",
    "- Aumentar a largura da \"zona de transição de zero para um\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos tentar ajustar uma função logística aos nossos dados dos alunos - essa é a base da regressão logística (que não é regressão, mas sim um método de classificação).\n",
    "\n",
    "![passou ou não prob](alunos_prob.png \"Probabilidade de aprovação versus número de horas de estudo\")\n",
    "\n",
    "Se tivermos que apostar se um aluno passa ou não de acordo com o número de horas de estudo deste, o melhor é adotar uma *regra de decisão* do tipo:\n",
    "\n",
    "- $\\hat{p}$ (probabilidade estimada) maior ou igual que $50\\%$: acho que passa.\n",
    "\n",
    "- $\\hat{p}$ menor que $50\\%$: acho que não passa.\n",
    "\n",
    "Temos agora um classificador de aluno! Eis o gráfico deste classificador em cima dos dados:\n",
    "\n",
    "![passou ou não class](alunos_class.png \"Vai passar ou não? versus número de horas de estudo\")\n",
    "\n",
    "Essa é a idéia da regressão logística (que não é regressão, mas sim um método de classificação). Vamos estudar agora mais a fundo esse modelo, e como fazer para descobrir os parâmetros da função logística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Atividade**\n",
    "\n",
    "- Qual o *precision*, *recall* e acurácia deste exemplo?\n",
    "\n",
    "- Se eu quisesse garantir uma chance de aprovação de mais de $80\\%$, quantas horas um aluno deveria estudar? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de regressão logística (que não é regressão, mas sim um método de classificação)\n",
    "\n",
    "Em um modelo de regressão linear estamos prevendo o valor da variável dependente. Em uma regressão logística (que não é regressão, mas sim um método de classificação), o que estamos tentando prever? Como se trata de um método de classificação, estamos tentando prever a classe $y$ de um objeto de atributos $\\mathbf{x}$. Esta classe deverá ser binária: zero ou um, negativo ou positivo. A regressão logística (que não é regressão, mas sim um método de classificação) atinge este objetivo da seguinte forma:\n",
    "\n",
    "- Para um conjunto de parâmetros $\\theta$, calcule a probabilidade (segundo o modelo) de que o objeto de atributos $\\mathbf{x}$ seja da classe positiva:\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\sigma(\\utilde{\\mathbf{x}}^{T} \\theta)\n",
    "$$\n",
    "\n",
    "Esta será a nossa função de decisão!\n",
    "\n",
    "(Como encontrar $\\theta$? Esse é o objetivo do algoritmo de treinamento, que vamos ver mais abaixo.)\n",
    "\n",
    "- Determine a classe do objeto usando o threshold $0.5$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\left\\{ \n",
    "\\begin{matrix}\n",
    "0, \\text{ se } \\hat{p} < 0.5 \\\\\n",
    "1, \\text{ se } \\hat{p} \\ge 0.5 \\\\\n",
    "\\end{matrix}\n",
    "\\right. \n",
    "$$\n",
    "\n",
    "## Função de custo\n",
    "\n",
    "Para obter o valor ótimo dos parâmetros $\\theta$ de um modelo de regressão logística temos que definir uma função de custo. Existem inúmeras possibilidades: basta escolher uma estratégia que penalize os erros e/ou valorize os acertos.\n",
    "\n",
    "Uma opção bastante conveniente (veremos depois porque) de função de custo para um dado objeto $(\\mathbf{x}, y)$ e um vetor de parâmetros $\\theta$ é a seguinte:\n",
    "\n",
    "- A probabilidade predita é $\\hat{p} = \\sigma(\\utilde{\\mathbf{x}}^T \\theta)$\n",
    "\n",
    "- Se a classe real $y$ for 1, a função de custo será $\\ell = -\\log{(\\hat{p})}$\n",
    "\n",
    "![](custo_y1.png)\n",
    "    \n",
    "- Se a classe real $y$ for 0, a função de custo será $\\ell = -\\log{(1 - \\hat{p})}$\n",
    "\n",
    "![](custo_y0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "**Atividade**\n",
    "\n",
    "Explique porque esta é uma função de custo que funciona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um jeito \"espertinho\" de escrever a função de custo de uma maneira que já incorpora tando o caso $y = 0$ como $y = 1$ é o seguinte:\n",
    "\n",
    "$$\n",
    "\\ell = - \\left(y \\log{(\\hat{p})} + (1 - y) \\log{(1 - \\hat{p})}\\right)\n",
    "$$\n",
    "\n",
    "Pois,\n",
    "\n",
    "- se $y = 1$, a função $\\ell$ simplifica para \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y = 1 \\Rightarrow \\ell \n",
    "& = - \\left( 1 \\log{(\\hat{p})} + (1 - 1) \\log{(1 - \\hat{p})}\\right) \\\\\n",
    "& = - \\left( 1 \\log{(\\hat{p})} + 0 \\log{(1 - \\hat{p})}\\right) \\\\\n",
    "& = - \\log{(\\hat{p})} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- se $y = 0$, a função $\\ell$ simplifica para \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y = 0 \\Rightarrow \\ell \n",
    "& = - \\left( 0 \\log{(\\hat{p})} + (1 - 0) \\log{(1 - \\hat{p})}\\right) \\\\\n",
    "& = - \\left( 0 \\log{(\\hat{p})} + 1 \\log{(1 - \\hat{p})}\\right) \\\\\n",
    "& = - \\log{(1 - \\hat{p})} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "A função de custo completa, para todas as amostras, é o custo médio por amostra:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) =\n",
    "- \\frac{1}{m} \\sum_{i = 1}^{m} \n",
    "\\left(\n",
    "    y^{(i)} \\log{\\left(\\hat{p}^{(i)}\\right)} + \n",
    "    \\left(1 - y^{(i)}\\right) \\left(\\log{\\left(1 - \\hat{p}^{(i)}\\right)}\\right)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Diferentemente do caso da regressão linear, aqui não temos uma solução fechada como a equação normal. Só nos resta o *gradient descent*. A boa notícia é que com essa função de custo as derivadas parciais são surpreendentemente simples:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_j} \\mathcal{L}(\\mathbf{\\theta}) =\n",
    "\\frac{1}{m} \n",
    "\\sum_{i = 1}^{m} \n",
    "\\left( \\hat{p}^{(i)} - y^{(i)} \\right)\n",
    "\\utilde{\\mathbf{x}}_{j}^{(i)}\n",
    "$$\n",
    "\n",
    "Em notação matricial:\n",
    "\n",
    "$$\\boxed{\n",
    "    \\nabla \\mathcal{L}(\\mathbf{\\theta}) =\n",
    "    \\frac{1}{m} \\utilde{\\mathbf{X}}^T \\left( \\hat{\\mathbf{p}} - \\mathbf{y} \\right)\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris: o verdadeiro \"Hello, world!\" dos modelos preditivos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(iris.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade**\n",
    "\n",
    "Construa um classificador por regressão logística para separar as flores do tipo 'Iris Versicolor' das demais usando as características 'petal length (cm)' e 'petal width (cm)'. Como resultado final, apresente:\n",
    "\n",
    "- Acurácia do classificador no conjunto de testes.\n",
    "- Curva ROC e respectiva área.\n",
    "- Um diagrama ilustrando a probabilidade da classe positiva. \n",
    "    - Dica: veja https://matplotlib.org/gallery/images_contours_and_fields/contour_demo.html\n",
    "\n",
    "Use seu arsenal de ferramentas de validação para encontrar o melhor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "X = X[['petal length (cm)', 'petal width (cm)']].values\n",
    "y = (y == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    random_state=RANDOM_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = X_train[y_train == 1]\n",
    "X_neg = X_train[y_train == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 'petal length (cm)'\n",
    "f2 = 'petal width (cm)'\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X_pos[:, 0], X_pos[:, 1], 'ro')\n",
    "plt.plot(X_neg[:, 0], X_neg[:, 1], 'bo')\n",
    "plt.xlabel(f1)\n",
    "plt.ylabel(f2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'accuracy: {100*accuracy_score(y_test, y_pred):.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict_proba(X_test)\n",
    "y_pred_prob = y_pred_prob[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f'AUC = {auc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, 'b-')\n",
    "plt.plot([0.0, 1.0], [0.0, 1.0], 'k--', alpha=0.2)\n",
    "plt.xlabel('FPR = 1 - specificity')\n",
    "plt.ylabel('TPR = sensitivity')\n",
    "plt.title(f'AUC = {auc:.3f}')\n",
    "plt.axis('square')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=1)),\n",
    "    ('clf', LogisticRegression(\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        max_iter=1000,\n",
    "    )),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'poly__degree': [1, 2, 3, 4],\n",
    "    'clf__C': [2**k for k in range(-10, 11)],\n",
    "    'clf__penalty': ['l1', 'l2']\n",
    "}\n",
    "clf = GridSearchCV(pipe, params, cv=5, n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'accuracy: {100*accuracy_score(y_test, y_pred):.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = clf.predict_proba(X_test)\n",
    "y_pred_prob = y_pred_prob[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f'AUC = {auc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, 'b-')\n",
    "plt.plot([0.0, 1.0], [0.0, 1.0], 'k--', alpha=0.2)\n",
    "plt.xlabel('FPR = 1 - specificity')\n",
    "plt.ylabel('TPR = sensitivity')\n",
    "plt.title(f'AUC = {auc:.3f}')\n",
    "plt.axis('square')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.arange(1.0, 7.0, 0.05)\n",
    "v = np.arange(0.0, 3.0, 0.05)\n",
    "U, V = np.meshgrid(u, v)\n",
    "\n",
    "X_test_plot = np.c_[U.reshape(U.size, 1), V.reshape(V.size, 1)]\n",
    "X_test_plot = pd.DataFrame(X_test_plot)\n",
    "y_test_plot = clf.predict_proba(X_test_plot)\n",
    "y_test_plot = y_test_plot[:, 1].reshape(U.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "CS = plt.contour(U, V, y_test_plot)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "X_test_pos = X_test[y_test]\n",
    "X_test_neg = X_test[~y_test]\n",
    "\n",
    "plt.plot(X_test_pos[:, 0], X_test_pos[:, 1], 'ro')\n",
    "plt.plot(X_test_neg[:, 0], X_test_neg[:, 1], 'bo')\n",
    "plt.xlabel(f1)\n",
    "plt.ylabel(f2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação multiclasse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressão linear é bacana, mas só serve para classificação binária. Se nosso problema for de classificação multiclasse, como proceder? Uma alternativa é recorrer às técnicas de \"One-Versus-One\" e \"One-Versus-All\" vista nas aulas passadas.\n",
    "\n",
    "Mas temos uma alternativa melhor aqui: podemos generalizar a técnica de regressão linear para a situação de várias classes: esta é a regressão linear multiclasse, ou regressão *softmax*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função softmax\n",
    "\n",
    "A regressão logística consiste em ajustar aos dados (via minimização da função de custo) uma curva logística. Isso pode ser quebrado em duas partes:\n",
    "\n",
    "- Calcular o *score* do objeto, que é $\\utilde{\\mathbf{x}}^T \\theta$\n",
    "\n",
    "- Passar o *score* pela função logística: $\\hat{p} = \\sigma(score)$\n",
    "\n",
    "E se tivéssemos várias classes? Uma possibilidade é a seguinte:\n",
    "\n",
    "- Para cada classe, calcule um *score* desta classe para o objeto: $s_k = \\utilde{\\mathbf{x}}_k^T \\theta_k$. Note que agora temos um vetor de parâmetros $\\theta_k$ por classe $k$.\n",
    "\n",
    "- Normalize esses *scores* com o auxílio da *função softmax*:\n",
    "\n",
    "$$\n",
    "\\hat{p}_k = \\sigma(s(\\mathbf{x}))_k = \\frac{\\exp\\left( s_k(\\mathbf{x}) \\right)}{\\sum_{j=1}^{K} \\exp\\left( s_j(\\mathbf{x}) \\right)}\n",
    "$$\n",
    "\n",
    "A classe atribuida ao objeto será então $\\arg \\max_k \\sigma(s(\\mathbf{x}))_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropia cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de custo da regressão *softmax* é muito parecida com a função de custo da regressão logística binária:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y = 0 \\Rightarrow & \\ell = -\\log{\\hat{p}_0} \\\\\n",
    "y = 1 \\Rightarrow & \\ell = -\\log{\\hat{p}_1} \\\\\n",
    "\\vdots & \\\\\n",
    "y = C - 1 \\Rightarrow & \\ell = -\\log{\\hat{p}_{(C - 1)}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Podemos novamente combinar as várias opções acima em uma única expressão:\n",
    "\n",
    "$$\n",
    "\\ell = - \\sum_{k = 0}^{C - 1} [y = k] \\log{(\\hat{p}_k)}\n",
    "$$\n",
    "\n",
    "onde a notação $[y = k]$ vale $1$ se a condição é verdadeira, e $0$ caso contrário. Esta notação chama-se \"colchetes de Iverson\" (\"*Iverson's brackets*\").\n",
    "\n",
    "Esta expressão lembra a fórmula da *entropia cruzada* entre $\\mathbf{y}$ e $\\mathbf{\\hat{p}}$. Esta é uma medida que vem da teoria da informação. Por conta desta coincidencia, esta perda é chamada de *perda de entropia cruzada*.\n",
    "\n",
    "A perda total sobre um *dataset* é o valor médio de $\\ell$ sobre todos os exemplos:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = - \\frac{1}{m} \\sum_{i = 1}^{m} \\sum_{k = 0}^{C - 1} \\left[y^{(i)} = k\\right] \\log{\\left(\\hat{p}_k^{(i)}\\right)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Repita a atividade de classificação do dataset 'Iris' usando apenas as características 'petal length (cm)' e 'petal width (cm)'. Use seu arsenal de ferramentas de validação para encontrar o melhor modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Repita a atividade anterior usando todas as quatro características originais. Qual o aumento de desempenho?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
